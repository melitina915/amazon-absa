{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMa00UcKxOGR8lZacpcaMQc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seo-won-lee/amazon-absa/blob/main/BARTABSA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNy69rNf9IIe"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/DACOS/absa_model/amazon_4_preprocess_lower.csv'"
      ],
      "metadata": {
        "id": "0lSTWAsm9JCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "xXxRIJok9LgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_colwidth', None)   # ÎòêÎäî 0 (Î≤ÑÏ†ÑÏóê Îî∞Îùº None ÎòêÎäî 0)\n",
        "df['review']"
      ],
      "metadata": {
        "id": "2GSTfIES9MJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ÎùºÏù¥Î∏åÎü¨Î¶¨ import\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "# ÏÇ¨Ï†Ñ ÌïôÏäµ Î™®Îç∏ ÏÇ¨Ïö©\n",
        "from datasets import Dataset\n",
        "# Hugging Face Îç∞Ïù¥ÌÑ∞ Íµ¨Ï°∞\n",
        "\n",
        "dataset = Dataset.from_pandas(df.reset_index(drop=True))\n",
        "# pandas -> Hugging Face Dataset Î≥ÄÌôò\n",
        "# Î≥ëÎ†¨ Ï≤òÎ¶¨ Î∞è Î∞∞Ïπò Ïó∞ÏÇ∞ ÏùòÎèÑ\n",
        "\n",
        "absa_pipe = pipeline(\n",
        "    # ABSA ÌååÏù¥ÌîÑÎùºÏù∏\n",
        "    task='ner',\n",
        "    # Named Entity Recognition Í∏∞Î∞ò ABSA ÏûëÏóÖ ÏàòÌñâ\n",
        "    model='gauneg/roberta-base-absa-ate-sentiment',\n",
        "    # Hugging FaceÏùò Aspect Term Extraction + Sentiment Î™®Îç∏ ÏÇ¨Ïö©\n",
        "    aggregation_strategy='simple',\n",
        "    # ÌÜ†ÌÅ∞ Îã®ÏúÑ ÏòàÏ∏°ÏùÑ Îã®Ïñ¥ Îã®ÏúÑÎ°ú Ìï©Ïπ®\n",
        "    device=0\n",
        "    # GPU Ïã§Ìñâ\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "def extract_absa(batch):\n",
        "    try:\n",
        "        preds_batch = absa_pipe(batch[\"review\"])\n",
        "        # Î∞∞Ïπò Î¶¨Ïä§Ìä∏ Ï†ÑÏ≤¥Î•º Î™®Îç∏Ïóê ÎÑ£Í≥† Ï∂îÎ°†\n",
        "    except Exception as e:\n",
        "        print(\"‚ùå Batch prediction failed:\", e)\n",
        "        preds_batch = [[] for _ in batch[\"review\"]]\n",
        "        # ÌÖçÏä§Ìä∏ Í∏∏Ïù¥ Ï¥àÍ≥º, GPU Out Of Memory Îì±Ïùò ÏòàÏô∏ Î∞úÏÉù Ïãú ÏóêÎü¨ Î©îÏãúÏßÄ Ï∂úÎ†•\n",
        "        # Ïã§Ìå®Ìïú Î¶¨Î∑∞Îì§Ïóê ÎåÄÌïú Î¶¨Ïä§Ìä∏ ÏÉùÏÑ±\n",
        "\n",
        "    aspects, sentiments = [], []\n",
        "    # Í∞Å Î¶¨Î∑∞Î≥ÑÎ°ú Ï∂îÏ∂úÌïú aspect, sentiment, score Ï†ÄÏû•\n",
        "\n",
        "    for preds in preds_batch:\n",
        "        # Î¶¨Î∑∞Î≥Ñ Í≤∞Í≥º Ï≤òÎ¶¨\n",
        "        preds = [r for r in preds if r[\"word\"].strip() != \".\" or len(r[\"word\"].strip()) > 2]\n",
        "        # ÌÜ†ÌÅ∞ Îã®ÏúÑÎ°ú ABSA Î™®Îç∏Ïùò Í≤∞Í≥º r Î∞òÌôò\n",
        "        # \".\" Îã®ÎèÖ ÌÜ†ÌÅ∞Í≥º Í∏∏Ïù¥Í∞Ä 1Ïù∏ Î¨¥ÏùòÎØ∏Ìïú Í≤∞Í≥º Ï†úÍ±∞ (\".\"Ïù¥ ÏïÑÎãàÍ±∞ÎÇò Í∏∏Ïù¥ 2 Ïù¥ÏÉÅÏù¥Î©¥ Ïú†ÏßÄ)\n",
        "        aspects.append([r[\"word\"].strip() for r in preds])\n",
        "        # aspect\n",
        "        sentiments.append([r[\"entity_group\"] for r in preds])\n",
        "        # Í∏çÏ†ï/Î∂ÄÏ†ï/Ï§ëÎ¶Ω Í∞êÏÑ± label\n",
        "\n",
        "    return {\n",
        "        \"aspect\": aspects,\n",
        "        \"sentiment\": sentiments\n",
        "        # \"score\": scores\n",
        "    }\n",
        "    # ÌÜµÌï©Îê† ÎîïÏÖîÎÑàÎ¶¨ Í≤∞Í≥º\n",
        "\n",
        "\n",
        "\n",
        "batch_sizes = [64, 48, 32, 16, 8]\n",
        "# ÌÅ∞ Î∞∞ÏπòÎ∂ÄÌÑ∞ ÏãúÎèÑÌïòÏó¨ Out Of MemoryÎ•º ÌîºÌïòÎ©∞ GPU Î©îÎ™®Î¶¨Î•º Ìö®Ïú®Ï†ÅÏúºÎ°ú ÏÇ¨Ïö©ÌïòÎèÑÎ°ù ÌïúÎã§\n",
        "result_dataset = None\n",
        "# Í≤∞Í≥º Ï†ÄÏû•\n",
        "\n",
        "for bs in batch_sizes:\n",
        "    # Î∞∞Ïπò Ï≤òÎ¶¨\n",
        "    # Î∞òÎ≥µÎ¨∏ÏúºÎ°ú ÌïòÎÇòÏî© batch_size Í∞í ÎÑ£ÏúºÎ©∞ Ïã§Ìñâ\n",
        "    try:\n",
        "        print(f\"üîç Trying batch_size={bs}\")\n",
        "        result_dataset = dataset.map(\n",
        "            # dataset.mapÏùÑ ÏÇ¨Ïö©Ìï¥ Î∞∞Ïπò Îã®ÏúÑÎ°ú extract_absa Ï†ÅÏö©\n",
        "            extract_absa,\n",
        "            batched=True,\n",
        "            # Î∞∞Ïπò Îã®ÏúÑ ÏûëÎèô\n",
        "            batch_size=bs,\n",
        "            desc=f\"Running ABSA (batch_size={bs})\"\n",
        "            # ÏßÑÌñâÎ•† ÌëúÏãú\n",
        "        )\n",
        "        print(f\"‚úÖ Success with batch_size={bs}\")\n",
        "        # ÏÑ±Í≥µÏãú Î£®ÌîÑ Ï¢ÖÎ£å\n",
        "        break\n",
        "    except RuntimeError as e:\n",
        "        print(f\"‚ùå Failed at batch_size={bs}: {e}\")\n",
        "        # Ïã§Ìå®Ïãú Îã§Ïùå ÏûëÏùÄ Î∞∞ÏπòÎ°ú ÎÑòÏñ¥Í∞ê\n",
        "        continue\n",
        "\n",
        "# Î™®Îì† batch_size Ïã§Ìå®Ìï† Í≤ΩÏö∞, Î©îÎ™®Î¶¨ Î¨∏Ï†úÎ°ú ÌåêÎã®ÌïòÍ≥† ÏóêÎü¨ Î∞úÏÉù\n",
        "if result_dataset is None:\n",
        "    raise RuntimeError(\"All batch sizes failed due to memory issues.\")\n",
        "\n",
        "# Hugging Face Dataset -> pandas DataFrame\n",
        "result_df = result_dataset.to_pandas()\n",
        "# pandas DataFrameÏóê Í≤∞Í≥º Î≥ëÌï©\n",
        "df[[\"aspect\", \"sentiment\"]] = result_df[[\"aspect\", \"sentiment\"]]\n",
        "\n",
        "# Í≤∞Í≥º ÌôïÏù∏\n",
        "df\n"
      ],
      "metadata": {
        "id": "JQsOJwgb8gQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('/content/drive/MyDrive/DACOS/absa_model/amazon_4_preprocess_lower_absa.csv', index=False, encoding='utf-8')"
      ],
      "metadata": {
        "id": "R65T9OndF5TY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FjXmbMDw9L2S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
